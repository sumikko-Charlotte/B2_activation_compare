# 实验思想日志

## 最初的想法

刚开始做这个实验时，我对激活函数的作用理解还比较浅显，只是知道它们能让神经网络具有非线性能力。我最初的想法很简单：既然 ReLU 在深度学习中这么流行，那它肯定是最好的，实验结果应该会证明这一点。我打算用 make_moons 数据集，因为它简单直观，容易可视化，适合做对比实验。

## 实验中遇到的问题

在编写代码的过程中，我遇到了一些小问题。首先是数据集的划分，一开始我忘记设置随机种子，导致每次运行结果都不一样，后来才意识到需要固定随机种子来保证可复现性。其次是损失曲线的绘制，刚开始我试图把所有激活函数的曲线放在一张图上，但颜色区分不够明显，调整了好几次才让图看起来清晰。另外，在训练过程中发现有些激活函数（比如 Sigmoid）的损失下降很慢，让我怀疑是不是学习率设置有问题。

## 对大模型预测的怀疑

大模型预测 ReLU 会表现最好，但我在实际训练时发现，对于 make_moons 这个相对简单的数据集，不同激活函数的差距可能没有想象中那么大。我开始怀疑，是不是在简单任务上，传统的 Sigmoid 或 Tanh 反而更稳定？毕竟它们的输出范围是有限的，可能对二分类问题更合适。而且，大模型的预测是基于一般性的经验，但具体到某个数据集，结果可能会有所不同。这让我意识到，理论预测和实际实验之间可能存在差距，只有通过实际验证才能得出可靠的结论。

