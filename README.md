# 激活函数对比实验

## 实验目的

本次实验的目的是比较不同激活函数在神经网络中的表现。我们使用 PyTorch 构建了一个简单的多层感知机（MLP），在 make_moons 数据集上对比了四种常见的激活函数：ReLU、Sigmoid、Tanh 和 LeakyReLU。通过观察训练过程和最终准确率，了解不同激活函数的特点。

## 运行方法

1. 确保已安装必要的 Python 库：
   ```
   pip install torch numpy scikit-learn matplotlib
   ```

2. 直接运行主程序：
   ```
   python main.py
   ```

3. 程序会自动进行训练，并在控制台输出训练进度。

## 输出说明

实验完成后，会在 `results/` 目录下生成两个文件：

- **summary.txt**：包含每种激活函数的最终验证集准确率，按准确率从高到低排序。
- **loss_curves.png**：训练过程中损失函数的变化曲线图，可以直观地看到不同激活函数的收敛情况。

## 实验设置

- 数据集：sklearn 的 make_moons（1000个样本，噪声0.2）
- 模型结构：3层全连接网络（2 → 64 → 32 → 2）
- 训练轮数：200 epochs
- 优化器：Adam（学习率 0.01）
- 损失函数：交叉熵损失

## 注意事项

- 实验使用固定随机种子（42），确保结果可复现
- 训练集和验证集按 8:2 划分
- 所有激活函数使用相同的网络结构和超参数，保证对比的公平性

